{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba62735",
   "metadata": {},
   "source": [
    "# 1.StopWords - Stemmer - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e315b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\himan\\anaconda3\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "yorumlar = pd.read_csv('Restaurant_Reviews.csv')\n",
    "yorumlar.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccee8a1",
   "metadata": {},
   "source": [
    "# Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab9833a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import re #regular expression\n",
    "#Preprocessing (Önişleme) -- SparceMatrix-StopWords-Stemmer\n",
    "derlem = []\n",
    "for i in range(len(yorumlar)):\n",
    "    yorum = re.sub('[^a-zA-Z]',' ',yorumlar['Review'][i])\n",
    "    yorum = yorum.lower()\n",
    "    yorum = yorum.split()\n",
    "    yorum = [ps.stem(kelime) for kelime in yorum if not kelime in set(stopwords.words('english'))] #stopwords.words('turkish')\n",
    "    yorum = ' '.join(yorum) #vektor için list değil string hali gerekli\n",
    "    derlem.append(yorum)\n",
    "    \n",
    "derlem[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bea70d",
   "metadata": {},
   "source": [
    "# 3.NLTK - Word2Vec(SkipGram,CBOW) - Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55f517",
   "metadata": {},
   "source": [
    "# NLTK-Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd727ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK-Tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Alan Mathison Turing was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74526a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan',\n",
       " 'Mathison',\n",
       " 'Turing',\n",
       " 'was',\n",
       " 'an',\n",
       " 'English',\n",
       " 'computer',\n",
       " 'scientist,',\n",
       " 'mathematician,',\n",
       " 'logician,']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c863f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan',\n",
       " 'Mathison',\n",
       " 'Turing',\n",
       " 'was',\n",
       " 'an',\n",
       " 'English',\n",
       " 'computer',\n",
       " 'scientist',\n",
       " ',',\n",
       " 'mathematician']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)[0:10] #kelıme tokenlestırme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "214f5e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan Mathison Turing was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist.',\n",
       " 'Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer.',\n",
       " 'Turing is widely considered to be the father of theoretical computer science and artificial intelligence.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)[0:10] #cumle tokenlestırme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b580ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan Mathison Turing was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist.',\n",
       " 'Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)[0:2] #cumle tokenlestırme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013b39f",
   "metadata": {},
   "source": [
    "# NLTK-StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ef2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# NLTK-StopWords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'Fazıl Say is a Turkish pianist and composer who was born in Ankara, described recently as \"not merely a pianist of genius; but undoubtedly he will be one of the great artists of the twenty-first century\".'\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "print(stopwords[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e5c08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fazıl',\n",
       " 'Say',\n",
       " 'Turkish',\n",
       " 'pianist',\n",
       " 'composer',\n",
       " 'born',\n",
       " 'Ankara',\n",
       " ',',\n",
       " 'described',\n",
       " 'recently']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "filtered_words[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014e6c3",
   "metadata": {},
   "source": [
    "# NLTK-Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599569b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n",
      "drive\n",
      "driver\n",
      "drive\n",
      "drove\n",
      "cat\n",
      "children\n"
     ]
    }
   ],
   "source": [
    "# NLTK-Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n",
    "#words = ['Boyunluk', 'Boynu', 'Boylar', 'Boyun', 'Boy']\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "    \n",
    "#kelimenin sonundaki ekler kesiliyor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9881f446",
   "metadata": {},
   "source": [
    "# NLTK-Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db385344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Friedrich',\n",
       " 'Wilhelm',\n",
       " 'Nietzsche',\n",
       " 'was',\n",
       " 'a',\n",
       " 'German',\n",
       " 'philosopher',\n",
       " ',',\n",
       " 'cultural',\n",
       " 'critic',\n",
       " ',',\n",
       " 'composer',\n",
       " ',',\n",
       " 'poet',\n",
       " ',',\n",
       " 'philologist',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'Latin',\n",
       " 'and',\n",
       " 'Greek',\n",
       " 'scholar',\n",
       " 'whose',\n",
       " 'work',\n",
       " 'has',\n",
       " 'exerted',\n",
       " 'a',\n",
       " 'profound',\n",
       " 'influence',\n",
       " 'on',\n",
       " 'Western',\n",
       " 'philosophy',\n",
       " 'and',\n",
       " 'modern',\n",
       " 'intellectual',\n",
       " 'history',\n",
       " '.',\n",
       " 'He',\n",
       " 'began',\n",
       " 'his',\n",
       " 'career',\n",
       " 'as',\n",
       " 'a',\n",
       " 'classical',\n",
       " 'philologist',\n",
       " 'before',\n",
       " 'turning',\n",
       " 'to',\n",
       " 'philosophy',\n",
       " '.',\n",
       " 'He',\n",
       " 'became',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'ever',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'the',\n",
       " 'Chair',\n",
       " 'of',\n",
       " 'Classical',\n",
       " 'Philology',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Basel',\n",
       " 'in',\n",
       " '1869',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '24',\n",
       " '.',\n",
       " 'Nietzsche',\n",
       " 'resigned',\n",
       " 'in',\n",
       " '1879',\n",
       " 'due',\n",
       " 'to',\n",
       " 'health',\n",
       " 'problems',\n",
       " 'that',\n",
       " 'plagued',\n",
       " 'him',\n",
       " 'most',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life',\n",
       " ';',\n",
       " 'he',\n",
       " 'completed',\n",
       " 'much',\n",
       " 'of',\n",
       " 'his',\n",
       " 'core',\n",
       " 'writing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'following',\n",
       " 'decade',\n",
       " '.',\n",
       " 'In',\n",
       " '1889',\n",
       " 'at',\n",
       " 'age',\n",
       " '44',\n",
       " ',',\n",
       " 'he',\n",
       " 'suffered',\n",
       " 'a',\n",
       " 'collapse',\n",
       " 'and',\n",
       " 'afterward',\n",
       " ',',\n",
       " 'a',\n",
       " 'complete',\n",
       " 'loss',\n",
       " 'of',\n",
       " 'his',\n",
       " 'mental',\n",
       " 'faculties',\n",
       " '.',\n",
       " 'He',\n",
       " 'lived',\n",
       " 'his',\n",
       " 'remaining',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'care',\n",
       " 'of',\n",
       " 'his',\n",
       " 'mother',\n",
       " 'until',\n",
       " 'her',\n",
       " 'death',\n",
       " 'in',\n",
       " '1897',\n",
       " 'and',\n",
       " 'then',\n",
       " 'with',\n",
       " 'his',\n",
       " 'sister',\n",
       " 'Elisabeth',\n",
       " 'Förster-Nietzsche',\n",
       " '.',\n",
       " 'Nietzsche',\n",
       " 'died',\n",
       " 'in',\n",
       " '1900',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK-Part of Speech Tagging\n",
    "import nltk\n",
    "\n",
    "text = 'Friedrich Wilhelm Nietzsche was a German philosopher, cultural critic, composer, poet, philologist, and a Latin and Greek scholar whose work has exerted a profound influence on Western philosophy and modern intellectual history. He began his career as a classical philologist before turning to philosophy. He became the youngest ever to hold the Chair of Classical Philology at the University of Basel in 1869 at the age of 24. Nietzsche resigned in 1879 due to health problems that plagued him most of his life; he completed much of his core writing in the following decade. In 1889 at age 44, he suffered a collapse and afterward, a complete loss of his mental faculties. He lived his remaining years in the care of his mother until her death in 1897 and then with his sister Elisabeth Förster-Nietzsche. Nietzsche died in 1900.'\n",
    "tokenized = nltk.word_tokenize(text)\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94846e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Friedrich', 'NNP'),\n",
       " ('Wilhelm', 'NNP'),\n",
       " ('Nietzsche', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('German', 'JJ'),\n",
       " ('philosopher', 'NN'),\n",
       " (',', ','),\n",
       " ('cultural', 'JJ'),\n",
       " ('critic', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized)[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169b51b",
   "metadata": {},
   "source": [
    "# \"\"\"\n",
    "CC     coordinating conjunction\n",
    "CD     cardinal digit\n",
    "DT     determiner\n",
    "EX     existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW     foreign word\n",
    "IN     preposition/subordinating conjunction\n",
    "JJ     adjective 'big'\n",
    "JJR    adjective, comparative 'bigger'\n",
    "JJS    adjective, superlative 'biggest'\n",
    "LS     list marker 1)\n",
    "MD     modal could, will\n",
    "NN     noun, singular 'desk'\n",
    "NNS    noun plural 'desks'\n",
    "NNP    proper noun, singular 'Harrison'\n",
    "NNPS   proper noun, plural 'Americans'\n",
    "PDT    predeterminer 'all the kids'\n",
    "POS    possessive ending parent's\n",
    "PRP    personal pronoun I, he, she\n",
    "PRP$   possessive pronoun my, his, hers\n",
    "RB     adverb very, silently,\n",
    "RBR    adverb, comparative better\n",
    "RBS    adverb, superlative best\n",
    "RP     particle give up\n",
    "TO     to go 'to' the store.\n",
    "UH     interjection errrrrrrrm\n",
    "VB     verb, base form take\n",
    "VBD    verb, past tense took\n",
    "VBG    verb, gerund/present participle taking\n",
    "VBN    verb, past participle taken\n",
    "VBP    verb, sing. present, non-3d take\n",
    "VBZ    verb, 3rd person sing. present takes\n",
    "WDT    wh-determiner which\n",
    "WP     wh-pronoun who, what\n",
    "WP$    possessive wh-pronoun whose\n",
    "WRB    wh-abverb where, when\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5ae3d",
   "metadata": {},
   "source": [
    "# NLTK-named entitiy recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0066e683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Steve', 'Jobs', 'was', 'an', 'American', 'entrepreneur', 'and', 'business', 'magnate', '.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK-named entitiy recognition\n",
    "\n",
    "import nltk\n",
    "text = \"Steve Jobs was an American entrepreneur and business magnate. He was the chairman, chief executive officer (CEO), and a co-founder of Apple Inc., chairman and majority shareholder of Pixar, a member of The Walt Disney Company's board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT. Jobs is widely recognized as a pioneer of the microcomputer revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak. \"\n",
    "tokenized = nltk.word_tokenize(text)\n",
    "print(tokenized[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa9d2452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), ('was', 'VBD'), ('an', 'DT'), ('American', 'JJ'), ('entrepreneur', 'NN'), ('and', 'CC'), ('business', 'NN'), ('magnate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "print(tagged[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9038bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('PERSON', [('Steve', 'NNP')]), Tree('PERSON', [('Jobs', 'NNP')]), ('was', 'VBD'), ('an', 'DT'), Tree('GPE', [('American', 'JJ')]), ('entrepreneur', 'NN'), ('and', 'CC'), ('business', 'NN'), ('magnate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "named_ent = nltk.ne_chunk(tagged)\n",
    "print(named_ent[0:10])\n",
    "#named_ent.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2354ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\himan\\anaconda3\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55e78c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\himan\\anaconda3\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c5bf1",
   "metadata": {},
   "source": [
    "# NLTK-Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee028d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n",
      "driving\n",
      "driver\n",
      "drive\n",
      "drove\n",
      "cat\n",
      "child\n",
      "drive\n"
     ]
    }
   ],
   "source": [
    "# NLTK-Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "words = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n",
    "for w in words:\n",
    "    print(lem.lemmatize(w))\n",
    "#kelimenin sözlükteki köküne iniliyor (morfolojik)\n",
    "\n",
    "print(lem.lemmatize('drove', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25350c5",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "516bff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['iran', 'devlet', 'televizyonu', 'ülkedeki', 'eyaletin', 'sinde', 'yapılan', 'reformcuları', 'protesto', 'amaçlı', 'yürüyüşlere', 'milyonlarca', 'kişinin', 'katıldığını', 'bildirdi'], ['gösterilerde', 'fitnecilere', 'ölüm', 'münafıklara', 'ölüm', 'abd', 'ye', 'ölüm', 'ingiltere', 'ye', 'ölüm', 'sloganları', 'atıldı'], ['dini', 'lider', 'ali', 'hamaney', 've', 'cumhurbaşkanı', 'mahmud', 'ahmedinejad', 'ı', 'destekleyen', 'iranlılar', 'son', 'olaylarda', 'yeğeni', 'öldürülen', 'mir', 'hüseyin', 'musevi', 'başta', 'olmak', 'üzere', 'muhalefet', 'liderlerini', 'kınadılar'], ['musevi', 'ye', 'ölüm', 've', 'idam', 'idam', 'sloganları', 'duyuldu'], ['muhalefet', 'liderleri', 'kaçtı', 'mı', 'aşure', 'günü', 'yaşanan', 'çatışmalarda', 'devlet', 'kaynaklarına', 'göre', 'u', 'terörist', 'olmak', 'üzere', 'kişi', 'ölmüştü'], ['den', 'fazla', 'kişinin', 'yaralandığı', 'olaylar', 'sırasında', 'en', 'az', 'kişi', 'tutuklanmıştı'], ['öte', 'yandan', 'iran', 'haber', 'ajansı', 'irna', 'muhalif', 'liderler', 'mir', 'hüseyin', 'musevi', 've', 'mehdi', 'kerrubi', 'nin', 'başkentten', 'kaçarak', 'ülkenin', 'kuzeyine', 'geçtiğini', 'ileri', 'sürdü', 'ancak', 'muhalefet', 'iddiayı', 'yalanladı'], ['hamaney', 'in', 'bir', 'dönem', 'korumalığını', 'yapan', 've', 'şu', 'anda', 'fransa', 'da', 'saklandığı', 'söylenen', 'bir', 'kişinin', 'muhalefete', 'verdiği', 'bilgilere', 'göre', 'münzevi', 'yaşamı', 'na', 'rağmen', 'dini', 'liderin', 'havyara', 'karşı', 'korkunç', 'bir', 'iştahı', 'var'], ['baston', 've', 'at', 'meraklısı', 'hamaney', 'aynı', 'zamanda', 'değerli', 'mücevherlerle', 'bezenmiş', 'bastonların', 've', 'cins', 'atların', 'koleksiyonunu', 'yapıyor'], ['hamaney', 'in', 'antika', 'bastonlarının', 'sayısı']]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec\n",
    "import numpy as np\n",
    "#numpy vektor ve matrısler uzerıne ıslem yapmamızı sağlar\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Word2Vec corpus üzerinde tüm kelimelerin üzerinden geç\n",
    "# her kelimenin etrafındaki kelimeleri tahmin et\n",
    "# iki kelime birbirne ne kadar sık bulunuyorsa vektöre yansıt\n",
    "f = open('hurriyet.txt', 'r', encoding='utf8')\n",
    "text = f.read()\n",
    "t_list = text.split('\\n')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for cumle in t_list:\n",
    "    corpus.append(cumle.split())\n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dfe527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.37132847e-01, -6.58725873e-02, -9.81643423e-02, -1.31674176e-02,\n",
       "        2.55890459e-01, -1.06994346e-01, -2.05155984e-01,  7.12770998e-01,\n",
       "        1.06691848e-02, -9.18999314e-02,  2.41413623e-01, -3.54081273e-01,\n",
       "       -1.54955357e-01,  2.54521936e-01, -2.40154296e-01,  3.29640418e-01,\n",
       "        4.91092473e-01, -4.12587017e-01,  2.46226102e-01, -4.85446036e-01,\n",
       "       -2.82664806e-01,  5.39066374e-01,  7.43631124e-01, -6.26489997e-01,\n",
       "       -3.41806442e-01,  9.56437960e-02, -4.33322489e-01, -1.99817680e-02,\n",
       "       -4.76657718e-01,  7.63881862e-01,  3.63801330e-01,  1.37588143e-01,\n",
       "       -1.20239303e-01, -5.59642255e-01,  1.85522422e-01, -1.28345668e-01,\n",
       "       -1.40270442e-01, -2.47893602e-01,  7.75754591e-03, -5.27047694e-01,\n",
       "        7.81673968e-01, -6.86972067e-02,  4.87782687e-01, -1.56884819e-01,\n",
       "        2.29207024e-01,  1.68537110e-01, -3.41241121e-01, -8.22828859e-02,\n",
       "        1.29675135e-01, -3.22596729e-01, -2.62458418e-02, -5.13421111e-02,\n",
       "        2.95533895e-01, -2.07136229e-01, -8.79068077e-02, -7.02717528e-02,\n",
       "       -1.23798989e-01, -7.39071593e-02, -7.79831409e-02, -5.34887373e-01,\n",
       "        2.55507827e-01,  9.25816074e-02, -3.70379955e-01,  2.19013989e-01,\n",
       "       -4.46748316e-01, -2.57578436e-02,  2.74731010e-01,  1.99130788e-01,\n",
       "       -3.44540536e-01, -2.89867222e-01, -1.61647126e-01,  3.05182427e-01,\n",
       "        4.23096776e-01, -2.70735532e-01,  3.92282248e-01, -2.06519917e-01,\n",
       "        1.29165899e-04, -3.34738977e-02, -3.85328420e-02,  3.87954991e-03,\n",
       "        4.40643162e-01, -1.12389982e+00,  3.59218456e-02,  3.01513225e-01,\n",
       "        1.90338820e-01,  5.55522032e-02,  5.19187987e-01,  3.97626221e-01,\n",
       "        6.98945165e-01,  5.57079196e-01,  3.52366447e-01, -8.98404047e-02,\n",
       "        2.18620136e-01,  1.10684402e-01,  4.50526863e-01,  1.52374446e-01,\n",
       "        2.37483233e-01,  1.28067240e-01,  3.47059399e-01,  6.59104660e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(corpus, window=5, min_count=5, sg=1) #genelde size 5-300 uzunlugunda vektor olustururuz #gozetımsız ogrenme yontemı\n",
    "#size 100 uzunlugunda vektor\n",
    "#window sol ve sagda bakılacak kelıme sayısı\n",
    "#sg 1 ise skip-gram kullanılacak, default olarak cbow kullanılıyor.\n",
    "#mın_count kelıme en az kac kere gecıyorsa al \n",
    "model.wv['ankara']\n",
    "# wv word vektorun kısaltılmısı\n",
    "#kelime vektörü, word vector, word embedding, embedding hep aynı şeyi ifade ediyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c5eda27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('avusturya', 0.8164887428283691), ('hollanda', 0.771075963973999), ('danimarka', 0.7710604667663574), ('fransa', 0.7591773271560669), ('belçika', 0.7384086847305298), ('bavyera', 0.7159342765808105), ('saksonya', 0.6963835954666138), ('alman', 0.6838916540145874), ('berlin', 0.6788915395736694), ('litvanya', 0.6677623391151428)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('almanya'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd6673",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4d1e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        word_vocab = set() # not using list to avoid duplicate entry\n",
    "        word2vector = {}\n",
    "        for line in f:\n",
    "            line_ = line.strip() #Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n",
    "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
    "    return word_vocab,word2vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30841c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
